{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c4428b",
   "metadata": {},
   "source": [
    "\n",
    "# Raman Spectra Analysis — Tutorial Notebook\n",
    "\n",
    "**Goal.** This is a *tutorial-style* notebook (not a model solution) that demonstrates a clean, reusable workflow for analyzing Raman spectra. It includes small, well-documented helper functions and uses **synthetic example data** you can regenerate to test the full pipeline.\n",
    "\n",
    "**What you'll learn / do here:**\n",
    "- Create synthetic Raman spectra for testing (multiple peaks, baseline, noise).\n",
    "- Pre-process spectra: cropping, cosmic-ray removal, smoothing, baseline correction, and normalization.\n",
    "- Detect candidate peaks and estimate initial parameters.\n",
    "- Fit peaks (Gaussian/Lorentzian/pseudo-Voigt) over selected regions.\n",
    "- Compute peak metrics (position, height, FWHM, area) and export results.\n",
    "- (Optional) Build a simple peak-ratio vs. concentration calibration using synthetic mixtures.\n",
    "\n",
    "> **Note:** Keep this as a *helper toolbox* for your own analysis. Adapt function parameters to your instrument and samples. Real data will require careful tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f2783",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Setup\n",
    "\n",
    "We use standard scientific Python libraries:\n",
    "- `numpy`, `scipy`, `matplotlib`\n",
    "- `pandas` for tidy result tables\n",
    "\n",
    "> No internet access is required; install these locally if missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0042d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import savgol_filter, find_peaks, medfilt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Matplotlib defaults (keep it simple; adjust as you like)\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "RNG = np.random.default_rng(42)  # for reproducibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5797654",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Synthetic Spectra Generator\n",
    "\n",
    "To prototype the analysis without needing files, we'll synthesize spectra with:\n",
    "- A wavenumber axis (e.g., 200–2000 cm⁻¹).\n",
    "- Several peaks (Gaussian, Lorentzian, or pseudo-Voigt).\n",
    "- A gentle baseline (polynomial) and additive noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f46a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gaussian(x, a, x0, sigma):\n",
    "    \"\"\"Gaussian peak; returns y.\"\"\"\n",
    "    return a * np.exp(-(x - x0)**2 / (2 * sigma**2))\n",
    "\n",
    "def lorentzian(x, a, x0, gamma):\n",
    "    \"\"\"Lorentzian peak; returns y.\"\"\"\n",
    "    return a * (gamma**2 / ((x - x0)**2 + gamma**2))\n",
    "\n",
    "def pseudo_voigt(x, a, x0, sigma, eta):\n",
    "    \"\"\"\n",
    "    Pseudo-Voigt as a weighted sum of Gaussian and Lorentzian.\n",
    "    - a: amplitude (peak height scale)\n",
    "    - x0: center\n",
    "    - sigma: Gaussian sigma\n",
    "    - eta: mixing (0..1), 0=Gaussian, 1=Lorentzian\n",
    "    \"\"\"\n",
    "    gauss = np.exp(-(x - x0)**2 / (2 * sigma**2))\n",
    "    # Convert sigma -> gamma approx for Lorentz part\n",
    "    gamma = sigma * np.sqrt(2*np.log(2))\n",
    "    lor = (gamma**2) / ((x - x0)**2 + gamma**2)\n",
    "    return a * (eta * lor + (1 - eta) * gauss)\n",
    "\n",
    "def poly_baseline(x, coeffs):\n",
    "    \"\"\"Evaluate polynomial baseline with given coefficients (np.polyval order).\"\"\"\n",
    "    return np.polyval(coeffs, x)\n",
    "\n",
    "def make_spectrum(x, peaks, baseline_coeffs=(0,0,0), noise_std=0.0):\n",
    "    \"\"\"\n",
    "    Build a synthetic spectrum.\n",
    "    - x: wavenumber axis\n",
    "    - peaks: list of dicts, each with:\n",
    "        {'kind': 'gaussian'|'lorentzian'|'pvoigt',\n",
    "         'a': amplitude, 'x0': center, 'w': width (sigma for gauss/pvoigt, gamma for lor),\n",
    "         'eta': mix for pvoigt (0..1) }\n",
    "    - baseline_coeffs: polynomial (highest first), e.g. (1e-7, -1e-3, 2)\n",
    "    - noise_std: additive Gaussian noise stdev\n",
    "    \"\"\"\n",
    "    y = np.zeros_like(x, dtype=float)\n",
    "    for p in peaks:\n",
    "        k = p.get('kind', 'gaussian').lower()\n",
    "        a = p['a']\n",
    "        x0 = p['x0']\n",
    "        w = p['w']\n",
    "        if k == 'gaussian':\n",
    "            y += gaussian(x, a, x0, w)\n",
    "        elif k == 'lorentzian':\n",
    "            y += lorentzian(x, a, x0, w)\n",
    "        elif k == 'pvoigt':\n",
    "            eta = float(p.get('eta', 0.5))\n",
    "            y += pseudo_voigt(x, a, x0, w, eta)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown peak kind: {k}\")\n",
    "    y += poly_baseline(x, baseline_coeffs)\n",
    "    if noise_std > 0:\n",
    "        y += RNG.normal(0, noise_std, size=x.shape)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b581f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example synthetic spectrum\n",
    "x = np.linspace(200, 2000, 5000)  # Raman shift [cm^-1]\n",
    "peaks = [\n",
    "    {'kind': 'gaussian', 'a': 1.5, 'x0': 520, 'w': 8},\n",
    "    {'kind': 'pvoigt',   'a': 1.0, 'x0': 1040, 'w': 10, 'eta': 0.3},\n",
    "    {'kind': 'lorentzian','a': 0.7, 'x0': 1450, 'w': 12},\n",
    "]\n",
    "baseline_coeffs = (1e-8, -2e-4, 6)  # gentle quadratic baseline\n",
    "y = make_spectrum(x, peaks, baseline_coeffs=baseline_coeffs, noise_std=0.03)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"Raman shift (cm$^{-1}$)\")\n",
    "plt.ylabel(\"Intensity (a.u.)\")\n",
    "plt.title(\"Synthetic Raman Spectrum (example)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f145803",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Pre-processing Helpers\n",
    "\n",
    "Real Raman data often needs:\n",
    "- **Cropping** to a region of interest (ROI)\n",
    "- **Cosmic-ray spike removal** (simple median filter or Hampel-like approach)\n",
    "- **Smoothing** with Savitzky–Golay (preserves peak shape)\n",
    "- **Baseline correction** (polynomial or asymmetric least squares)\n",
    "- **Normalization** (max, area, or vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbacde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def crop(x, y, xmin=None, xmax=None):\n",
    "    m = np.ones_like(x, dtype=bool)\n",
    "    if xmin is not None:\n",
    "        m &= x >= xmin\n",
    "    if xmax is not None:\n",
    "        m &= x <= xmax\n",
    "    return x[m], y[m]\n",
    "\n",
    "def remove_cosmic_rays(y, kernel_size=3, thresh=6.0):\n",
    "    \"\"\"\n",
    "    Simple spike suppressor: compare to median-filtered signal.\n",
    "    Replace outliers that exceed 'thresh' * local MAD (approx) with median value.\n",
    "    \"\"\"\n",
    "    y_med = medfilt(y, kernel_size=kernel_size)\n",
    "    resid = y - y_med\n",
    "    mad = np.median(np.abs(resid - np.median(resid))) + 1e-12\n",
    "    mask = np.abs(resid) > (thresh * 1.4826 * mad)\n",
    "    y_fix = y.copy()\n",
    "    y_fix[mask] = y_med[mask]\n",
    "    return y_fix\n",
    "\n",
    "def smooth_savgol(y, window_length=21, polyorder=3):\n",
    "    if window_length % 2 == 0:\n",
    "        window_length += 1  # must be odd\n",
    "    return savgol_filter(y, window_length=window_length, polyorder=polyorder)\n",
    "\n",
    "def baseline_polyfit(x, y, deg=2, mask_sigma=2.5, iters=3):\n",
    "    \"\"\"\n",
    "    Robust polynomial baseline fit with iterative sigma-masking.\n",
    "    Returns baseline estimate and (y - baseline).\n",
    "    \"\"\"\n",
    "    mask = np.ones_like(y, dtype=bool)\n",
    "    for _ in range(iters):\n",
    "        coeffs = np.polyfit(x[mask], y[mask], deg)\n",
    "        base = np.polyval(coeffs, x)\n",
    "        resid = y - base\n",
    "        s = np.std(resid[mask])\n",
    "        mask = resid < (mask_sigma * s)\n",
    "    coeffs = np.polyfit(x[mask], y[mask], deg)\n",
    "    base = np.polyval(coeffs, x)\n",
    "    return base, y - base\n",
    "\n",
    "def normalize(y, method=\"max\"):\n",
    "    if method == \"max\":\n",
    "        scale = np.max(y) if np.max(y) != 0 else 1.0\n",
    "    elif method == \"area\":\n",
    "        scale = np.trapz(np.abs(y)) or 1.0\n",
    "    elif method == \"vector\":\n",
    "        scale = np.linalg.norm(y) or 1.0\n",
    "    else:\n",
    "        raise ValueError(\"Unknown normalization method\")\n",
    "    return y / scale\n",
    "\n",
    "def calibrate_axis(x_pixels, coeffs):\n",
    "    \"\"\"\n",
    "    Map pixel/channel axis -> Raman shift via polynomial coeffs (highest power first).\n",
    "    Placeholder to be used with real coefficients from 'calibration_hg_na.ipynb'.\n",
    "    \"\"\"\n",
    "    return np.polyval(coeffs, x_pixels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8972c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Demonstrate preprocessing on the example synthetic spectrum\n",
    "x_roi, y_roi = crop(x, y, 350, 1700)\n",
    "y_despike = remove_cosmic_rays(y_roi, kernel_size=5, thresh=5.0)\n",
    "y_smooth  = smooth_savgol(y_despike, window_length=17, polyorder=3)\n",
    "base, y_corr = baseline_polyfit(x_roi, y_smooth, deg=2, mask_sigma=2.0, iters=3)\n",
    "y_norm = normalize(y_corr, method=\"max\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_roi, y_roi, label=\"raw (ROI)\")\n",
    "ax.plot(x_roi, y_despike, label=\"despiked\")\n",
    "ax.plot(x_roi, y_smooth, label=\"smoothed\")\n",
    "ax.plot(x_roi, base, label=\"baseline (fit)\")\n",
    "ax.set_xlabel(\"Raman shift (cm$^{-1}$)\")\n",
    "ax.set_ylabel(\"Intensity (a.u.)\")\n",
    "ax.set_title(\"Preprocessing steps\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_roi, y_norm)\n",
    "plt.xlabel(\"Raman shift (cm$^{-1}$)\")\n",
    "plt.ylabel(\"Normalized Intensity (a.u.)\")\n",
    "plt.title(\"Baseline-corrected & normalized\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4778e922",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Peak Detection & Initial Guesses\n",
    "\n",
    "Use `scipy.signal.find_peaks` with thresholds on height, prominence, and minimum distance. Then estimate initial parameters for fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d664605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_sigma_from_width(x, peak_index, y, rel_height=0.5):\n",
    "    \"\"\"\n",
    "    Rough sigma estimate from local width at rel_height (FWHM ~ 2.355*sigma for Gaussian).\n",
    "    \"\"\"\n",
    "    peak_x = x[peak_index]\n",
    "    peak_y = y[peak_index]\n",
    "    target = peak_y * rel_height\n",
    "    # left\n",
    "    i = peak_index\n",
    "    while i > 0 and y[i] > target:\n",
    "        i -= 1\n",
    "    left_x = x[i]\n",
    "    # right\n",
    "    j = peak_index\n",
    "    while j < len(y)-1 and y[j] > target:\n",
    "        j += 1\n",
    "    right_x = x[j]\n",
    "    fwhm = max(1e-9, right_x - left_x)\n",
    "    sigma = fwhm / 2.355\n",
    "    return max(1.0, sigma)  # keep a minimum width\n",
    "\n",
    "def find_candidate_peaks(x, y, height=None, prominence=0.02, distance=30):\n",
    "    peaks_idx, props = find_peaks(y, height=height, prominence=prominence, distance=distance)\n",
    "    results = []\n",
    "    prominences = props.get(\"prominences\", np.zeros_like(peaks_idx, dtype=float))\n",
    "    for k, idx in enumerate(peaks_idx):\n",
    "        sigma0 = estimate_sigma_from_width(x, idx, y, rel_height=0.5)\n",
    "        results.append({\n",
    "            \"index\": int(idx),\n",
    "            \"x0\": float(x[idx]),\n",
    "            \"height\": float(y[idx]),\n",
    "            \"sigma0\": float(sigma0),\n",
    "            \"prominence\": float(prominences[k] if k < len(prominences) else 0.0),\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd940600",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cands = find_candidate_peaks(x_roi, y_norm, prominence=0.05, distance=40)\n",
    "pd.DataFrame(cands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f9efd",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Peak Fitting\n",
    "\n",
    "Fit one or more peaks in a region. We'll implement Gaussian, Lorentzian, and pseudo-Voigt models with `scipy.optimize.curve_fit`.\n",
    "\n",
    "> In practice, constrain parameters carefully and fit small windows around peaks of interest for robustness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df624de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gaussian_sum(x, *params):\n",
    "    \"\"\"\n",
    "    Sum of N Gaussians.\n",
    "    params = [a1, x01, sigma1, a2, x02, sigma2, ...]\n",
    "    \"\"\"\n",
    "    y = np.zeros_like(x, dtype=float)\n",
    "    for i in range(0, len(params), 3):\n",
    "        a, x0, s = params[i:i+3]\n",
    "        y += gaussian(x, a, x0, s)\n",
    "    return y\n",
    "\n",
    "def lorentzian_sum(x, *params):\n",
    "    y = np.zeros_like(x, dtype=float)\n",
    "    for i in range(0, len(params), 3):\n",
    "        a, x0, g = params[i:i+3]\n",
    "        y += lorentzian(x, a, x0, g)\n",
    "    return y\n",
    "\n",
    "def pvoigt_sum(x, *params):\n",
    "    \"\"\"\n",
    "    Sum of N pseudo-Voigts.\n",
    "    params = [a1, x01, sigma1, eta1, a2, x02, sigma2, eta2, ...]\n",
    "    \"\"\"\n",
    "    y = np.zeros_like(x, dtype=float)\n",
    "    for i in range(0, len(params), 4):\n",
    "        a, x0, s, eta = params[i:i+4]\n",
    "        y += pseudo_voigt(x, a, x0, s, eta)\n",
    "    return y\n",
    "\n",
    "def fit_region(x, y, model=\"gaussian\", x_min=None, x_max=None, init_peaks=None, bounds=None):\n",
    "    \"\"\"\n",
    "    Fit peaks in [x_min, x_max] region.\n",
    "    - model: 'gaussian'|'lorentzian'|'pvoigt'\n",
    "    - init_peaks: list of dicts [{'a':..., 'x0':..., 'w':..., 'eta':...}]\n",
    "    - bounds: tuple (lower, upper) arrays matching parameter length\n",
    "    Returns popt, pcov, x_fit, y_fit\n",
    "    \"\"\"\n",
    "    xm, ym = crop(x, y, x_min, x_max)\n",
    "    if init_peaks is None or len(init_peaks) == 0:\n",
    "        raise ValueError(\"Provide at least one initial peak guess in init_peaks.\")\n",
    "\n",
    "    if model == \"gaussian\":\n",
    "        p0 = []\n",
    "        for p in init_peaks:\n",
    "            p0 += [p['a'], p['x0'], max(1.0, p['w'])]\n",
    "        f = gaussian_sum\n",
    "    elif model == \"lorentzian\":\n",
    "        p0 = []\n",
    "        for p in init_peaks:\n",
    "            p0 += [p['a'], p['x0'], max(1.0, p['w'])]\n",
    "        f = lorentzian_sum\n",
    "    elif model == \"pvoigt\":\n",
    "        p0 = []\n",
    "        for p in init_peaks:\n",
    "            p0 += [p['a'], p['x0'], max(1.0, p['w']), np.clip(p.get('eta', 0.5), 0.0, 1.0)]\n",
    "        f = pvoigt_sum\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "\n",
    "    popt, pcov = curve_fit(f, xm, ym, p0=p0, bounds=bounds if bounds is not None else (-np.inf, np.inf), maxfev=10000)\n",
    "    yfit = f(xm, *popt)\n",
    "    return popt, pcov, xm, yfit\n",
    "\n",
    "def peak_metrics_gaussian(a, sigma):\n",
    "    \"\"\"\n",
    "    For Gaussian: height = a, area = a * sigma * sqrt(2*pi), FWHM = 2.355*sigma\n",
    "    \"\"\"\n",
    "    area = a * sigma * np.sqrt(2*np.pi)\n",
    "    fwhm = 2.355 * sigma\n",
    "    return {\"height\": a, \"area\": area, \"fwhm\": fwhm}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac774264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Fit the ~1040 cm^-1 region with a single pseudo-Voigt\n",
    "x_min, x_max = 990, 1090\n",
    "init = [{\"a\": 0.9, \"x0\": 1040, \"w\": 8, \"eta\": 0.3}]  # rough guesses\n",
    "\n",
    "popt, pcov, xf, yf = fit_region(x_roi, y_norm, model=\"pvoigt\", x_min=x_min, x_max=x_max, init_peaks=init)\n",
    "\n",
    "plt.plot(x_roi, y_norm, alpha=0.6, label=\"data\")\n",
    "plt.plot(xf, yf, label=\"fit\")\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.xlabel(\"Raman shift (cm$^{-1}$)\")\n",
    "plt.ylabel(\"Normalized Intensity (a.u.)\")\n",
    "plt.title(\"Fit around ~1040 cm$^{-1}$ (pseudo-Voigt)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "popt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d122535",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Simple Quantification Example (Synthetic Mixtures)\n",
    "\n",
    "We'll generate mixtures of two components (A & B) with peaks around 880 and 1040 cm⁻¹.\n",
    "We'll then compute a **peak area ratio** and see how it correlates with the mixture fraction.\n",
    "This is purely illustrative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a3c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Synthetic generator (mit optionalem internem Standard) + Fit-Helper + R² ---\n",
    "\n",
    "def synth_mix_spectrum(x, frac_A, noise_std=0.003, baseline_coeffs=(0, 0, 0), include_is=True):\n",
    "    \"\"\"\n",
    "    Linear gemischtes Spektrum aus A und B.\n",
    "      - A: starker Peak @ ~880\n",
    "      - B: starker Peak @ ~1040\n",
    "      - Optionaler interner Standard (IS) @ ~1000 mit konstanter Intensität.\n",
    "    Es gibt KEINE Normalisierung, damit die Flächen linear mit der \"Konzentration\" skalieren.\n",
    "    \"\"\"\n",
    "    # Fixe, identische Breiten für saubere Flächenproportionalität\n",
    "    wA, wB, wIS = 8.0, 9.0, 8.0\n",
    "\n",
    "    peaks_A = [\n",
    "        {'kind': 'gaussian',  'a': 1.20*frac_A,       'x0': 880.0,  'w': wA},\n",
    "        {'kind': 'gaussian',  'a': 0.60*frac_A,       'x0': 1450.0, 'w': 10.0},\n",
    "    ]\n",
    "    peaks_B = [\n",
    "        {'kind': 'pvoigt',    'a': 1.20*(1-frac_A),   'x0': 1040.0, 'w': wB, 'eta': 0.3},\n",
    "        {'kind': 'lorentzian','a': 0.50*(1-frac_A),   'x0': 1300.0, 'w': 12.0},\n",
    "    ]\n",
    "    peaks_IS = []\n",
    "    if include_is:\n",
    "        peaks_IS = [{'kind': 'gaussian', 'a': 1.00, 'x0': 1000.0, 'w': wIS}]  # konstant!\n",
    "\n",
    "    yA = make_spectrum(x, peaks_A, baseline_coeffs=(0, 0, 0), noise_std=0.0)\n",
    "    yB = make_spectrum(x, peaks_B, baseline_coeffs=(0, 0, 0), noise_std=0.0)\n",
    "    yIS = make_spectrum(x, peaks_IS, baseline_coeffs=(0, 0, 0), noise_std=0.0) if include_is else 0.0\n",
    "\n",
    "    y = yA + yB + yIS + poly_baseline(x, baseline_coeffs)\n",
    "    if noise_std and noise_std > 0:\n",
    "        y += RNG.normal(0, noise_std, size=x.shape)\n",
    "    return y\n",
    "\n",
    "def area_from_gaussian_fit(x, y, center, half_window=15, a0=0.8, w0=8.0):\n",
    "    \"\"\"\n",
    "    Fit eines einzelnen Gaussians im kleinen Fenster um 'center' und Ausgabe der Fläche.\n",
    "    Achtung: für pseudo-Voigt/Lorentz brauchst du eigene Flächen-Formeln.\n",
    "    \"\"\"\n",
    "    popt, pcov, xf, yf = fit_region(\n",
    "        x, y, model=\"gaussian\",\n",
    "        x_min=center-half_window, x_max=center+half_window,\n",
    "        init_peaks=[{\"a\": a0, \"x0\": center, \"w\": w0}],\n",
    "        bounds=None\n",
    "    )\n",
    "    a, x0, sigma = popt[:3]\n",
    "    return float(a * sigma * np.sqrt(2*np.pi))  # Gaussian area\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"Einfaches Bestimmtheitsmaß R².\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true))**2) + 1e-15\n",
    "    return 1.0 - ss_res/ss_tot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calibration with internal standard (A/IS vs Fraction A) ---\n",
    "\n",
    "fractions = np.linspace(0, 1, 9)\n",
    "areas_A, areas_IS = [], []\n",
    "\n",
    "for f in fractions:\n",
    "    y = synth_mix_spectrum(x, frac_A=f, noise_std=0.003, baseline_coeffs=(0,0,0), include_is=True)\n",
    "    x_r, y_r = crop(x, y, 350, 1700)\n",
    "\n",
    "    A  = area_from_gaussian_fit(x_r, y_r, center=880.0,  half_window=15, a0=1.0, w0=8.0)\n",
    "    IS = area_from_gaussian_fit(x_r, y_r, center=1000.0, half_window=15, a0=1.0, w0=8.0)\n",
    "\n",
    "    areas_A.append(A)\n",
    "    areas_IS.append(IS)\n",
    "\n",
    "df_cal = pd.DataFrame({\n",
    "    \"fraction_A\": fractions,\n",
    "    \"area_A_880\": areas_A,\n",
    "    \"area_IS_1000\": areas_IS,\n",
    "    \"ratio_A_over_IS\": np.array(areas_A) / (np.array(areas_IS) + 1e-12),\n",
    "})\n",
    "\n",
    "# Lineare Kalibration\n",
    "def linear(x, m, b): return m*x + b\n",
    "\n",
    "y_ratio = df_cal[\"ratio_A_over_IS\"].values\n",
    "popt, _ = curve_fit(linear, df_cal[\"fraction_A\"], y_ratio)\n",
    "yfit = linear(df_cal[\"fraction_A\"], *popt)\n",
    "r2 = r2_score(y_ratio, yfit)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(df_cal[\"fraction_A\"], y_ratio, label=\"data\")\n",
    "xf = np.linspace(0, 1, 200)\n",
    "plt.plot(xf, linear(xf, *popt), label=f\"fit (R²={r2:.2f})\")\n",
    "plt.xlabel(\"Fraction A (synthetic)\")\n",
    "plt.ylabel(\"Area ratio A(880) / IS(1000)\")\n",
    "plt.title(\"Calibration: A/IS — linear with internal standard\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Calibration (A/IS) — slope, intercept, R² = \", popt[0], popt[1], round(r2,2))\n",
    "\n",
    "\n",
    "# Optional: Tabelle inspizieren\n",
    "df_cal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8155834",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Export Results\n",
    "\n",
    "Export tables as CSV for further analysis or reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977bf7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_path = \"calibration_example_results.csv\"\n",
    "df_cal.to_csv(out_path, index=False)\n",
    "out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022d3dbf",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Next Steps & Tips\n",
    "\n",
    "- Replace synthetic data with your real spectra (CSV or vendor export).\n",
    "- If you have pixel/channel axis, apply your calibration (e.g., from `calibration_hg_na.ipynb`).\n",
    "- For robust fits:\n",
    "  - Work in small windows around peaks of interest.\n",
    "  - Constrain parameters (bounds) and provide good initial guesses.\n",
    "  - Use replicate spectra to estimate uncertainty.\n",
    "- Consider more advanced baselines (e.g., asymmetric least squares) or deconvolution if peaks are overlapped.\n",
    "- Document your chosen parameters in a lab notebook to keep analysis reproducible.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
